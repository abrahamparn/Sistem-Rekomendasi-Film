# -*- coding: utf-8 -*-
"""A Movie Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TL5tG_DRjRIHyBd6gYWM_QVVcnkZme1E

# Just Another Movie Recommendation System


---
Oleh: [Abraham Naiborhu](https://www.dicoding.com/users/abrahampn)

*Proyek Final - Dicoding Machine Learning Terapan*

## Pendahuluan
Proyek ini membahas topik rekomendasi sistem, terkhususnya untuk merekomendasikan film kepada users. Dataset ini diambil dari kaggle:  https://www.kaggle.com/aigamer/movie-lens-dataset

Sistem rekomendasi yang digunakan disini adalah content dan collaborative based filtering
* Pada Content Based Filter, kita akan menggunakan genre sebagai pusat dari sistem rekomendasi

* Pada Collaborative Based Filter, kita akan menggunakan rating dari pengguna

## Data and Libraries Preparation

Saat menggunakan Opendatasets, diharapkan pengguna menyiapkan kaggle (karena dataset dari kaggle) username dan kaggle key
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install opendatasets

import opendatasets as od
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""Setelah importing libraries, waktunya mempersiapkan dataset"""

od.download("https://www.kaggle.com/aigamer/movie-lens-dataset")
df_movie = pd.read_csv("/content/movie-lens-dataset/movies.csv")
df_rating = pd.read_csv("/content/movie-lens-dataset/ratings.csv")
df_tags = pd.read_csv("/content/movie-lens-dataset/tags.csv")
df_links = pd.read_csv("/content/movie-lens-dataset/links.csv")

"""## Data Understanding
Bagian ini akan menjelaskan mengenai karakteristik dari dataset yang didapatkan.


Pada dataset ini, terdapat empat file csv yang kelak akan digunakan untuk pembuatan model. Ke empat file tersebut adalah: links.csv, ratings.csv, movies.csv, and tags.csv

There are a total of 9742 Movie's in our dataset with 100836 Users Rating the movie and 3683 Tags.
* userId: Unique Id provided for each User
  * userId were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).

* movieId: Unique Id provided for each Movie
  * Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id 1 corresponds to the URL Movie Lens. Movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).

* rating (rating.csv): Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).
  * All Ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user
  * Mean show the average rating is 3.2-Star
  
* genres: Genres are a pipe-separated list, and are selected from the following:
  * Action
  * Adventure
  * Animation
  * Children's
  * Comedy
  * Crime
  * Documentary
  * Drama
  * Fantasy
  * Film-Noir
  * Horror
  * Musical
  * Mystery
  * Romance
  * Sci-Fi
  * Thriller
  * War
  * Western
  * (no genres listed)

### All About df_Movie
"""

df_movie

df_movie.info()

df_movie.describe()

"""### All About df_rating"""

df_rating

df_rating.info()

print('Bintang yang diberikan: ', df_rating.rating.unique())

df_rating.describe()

"""### All About df_tags"""

df_tags

df_tags.info()

df_tags.describe()

"""### All About df_links"""

df_links

df_links.info()

df_links.describe()

"""### Summary of All Data

Setelah penelusuran diatas, kita mengetahui bahwa data yang akan digunakan hanya berdasarkan dari dua file, yaitu rating.csv dan file movie.csv, file-file lainnya tidak akan kita gunakan. Kemudian, setelah melihat hasil dari kedua file, kita dapat menyimpulkan variable variabe yang akan digunakan adalah sebagai berikut:

* movie.csv
  * MovieID: Data ID Film (int64)
  * Title: Judul Film (Object)
  * Genres: Genre Film (Object)

* rating.csv
  * UserID: Data ID pengguna (int64)
  * MoveID: Data ID Film (int64)
  * Rating: Penilaian pengguna terhadap film (float64)
  * Timestamp: timestamp (int64)

Note: Penjelasan lebih mendetail mengenai variabel ada di pembukaan sub bab ini
"""

#Perbedaan Jumlah film di masing-masing file data
print('Jumlah Film di df_movie: ', len(df_movie.movieId.unique()))
print('Jumlah Film di df_rating: ', len(df_rating.movieId.unique()))
print('Jumlah Film di df_tags: ', len(df_tags.movieId.unique()))
print('Jumlah Film di df_links: ', len(df_links.movieId.unique()))
print('\n')

#Perbedaan jumlah film di masing-masing file data
print('Jumlah User di df_rating', len(df_rating.userId.unique()))
print('Jumlah User di df_tags', len(df_tags.userId.unique()))

# Mengecek jumlah user
user_all = np.concatenate((
    df_rating.userId.unique(),
    df_tags.userId.unique(),
))

user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user berdasarkan placeID: ', len(user_all))

# Mengecek jumlah Movie
movie_all = np.concatenate((
    df_movie.movieId.unique(),
    df_rating.movieId.unique(),
    df_links.movieId.unique()
))

movie_all = np.sort(np.unique(movie_all))
print('Jumlah seluruh movie berdasarkan moveId: ', len(movie_all))

# Memisahkan column tahun untuk visualisasi tahun
df_movie['year'] = df_movie['title'].str.extract('(\(\d+\))')
df_movie['year'] = df_movie['year'].str.extract('(\d+)').astype(float)
df_movie = df_movie.sort_values( by = 'year', ascending = False)
df_movie

#Memvisualisasikan tahun
plt.figure(figsize=(20,8))
sns.histplot(data = df_movie, x = 'year', bins = 200)
plt.show()

# Mengambil column genres dan melakukan encoding
genres = []
for i in range(len(df_movie.genres)):
  for x in df_movie.genres[i].split('|'):
    if x not in genres:
      genres.append(x)

len(genres)
for x in genres:
    df_movie[x] = 0
for i in range(len(df_movie.genres)):
    for x in df_movie.genres[i].split('|'):
        df_movie[x][i]=1

df_movie

#memvisualisasikan genre
genre_visualization = df_movie.drop('year', axis = 1)
genre_visualization = genre_visualization.iloc[:,3:].sum().reset_index()
genre_visualization.columns = ['genre','total'] 
print(genre_visualization)
print('\n')
print(genres)

#Masih memvisualisasikan Genre
plt.figure(figsize=(12,6))
sns.barplot(x = 'genre', y = 'total', data = genre_visualization, palette = 'Set3')
plt.xticks(rotation = 90)
plt.plot

#Menggabungkan dua dataframe untuk visualisasi rating tertinggi per title
data = pd.merge(df_movie, df_rating, on = 'movieId')
data = data.drop(genres, axis = 1)
data

# Melihat rating per-title tertinggi
visualisasi = data.groupby('title')['rating'].count().reset_index().rename(columns = {'rating': 'total_rating'})
visualisasi = visualisasi.sort_values(by = 'total_rating', ascending = False)
visualisasi.head(10)

#visualisasi 10 titel dengan rating terbanyak
plt.figure(figsize=(12,6))
sns.barplot(x = 'title', y = 'total_rating', data = visualisasi.iloc[:10,:], palette = "Set3")
plt.xticks(rotation = 90)
plt.plot

# Visualisasi rating dari rating dataframe
rating = df_rating['rating'].value_counts()
rating.plot(kind = 'bar', title = "Rating")
plt.show()

"""## Data Preparation

Hal yang akan digunakan untuk data preparation ini adalah#
1. Mengecek dan menghilangkan data duplikat
2. Menghilangkan data Null
3. Menghilangkan variabel yang tidak dibutuhkan lagi
4. Preparation for Cosine Similarity
5. Preparation for Deep Learning
"""

#Cek data duplikat
print(df_movie[df_movie.duplicated()])
print(df_rating[df_rating.duplicated()])
print(df_tags[df_tags.duplicated()])
print(df_links[df_links.duplicated()])

#Cek data null
print(df_movie.isnull().sum())
print('\n')
print(df_rating.isnull().sum())
print('\n')
print(df_tags.isnull().sum())
print('\n')
print(df_links.isnull().sum())

#Karena kita tidak membutuhkan column Tahun, kita drop column tahun

df_movie = df_movie.drop('year', axis = 1)

#data_preparation for cosine similarity

from scipy.sparse import csr_matrix
df_cosine = df_movie.drop(['movieId', 'genres'], axis = 1)
df_cosine = df_cosine.set_index('title')
df_cosine = csr_matrix(df_cosine.values)
print(df_cosine)

#Data Preparation for deep learning

#User_ID Prep
user_id = data['userId'].unique().tolist()
user_to_encoded = {x: i for i, x in enumerate(user_id)}
encoded_to_user = {i: x for i, x in enumerate(user_id)}

#Movie_ID Prep
movie_id = data['movieId'].unique().tolist()
movie_to_encoded = {x: i for i, x in enumerate(movie_id)}
encoded_to_movie = {i: x for i, x in enumerate(movie_id)}

#mapping
data['user'] = data['userId'].map(user_to_encoded)
data['movie'] = data['movieId'].map(movie_to_encoded)

#visualize
data

# Mendapatkan jumlah user
num_users = len(user_to_encoded)
print(num_users)
 
# Mendapatkan jumlah movie
num_movie = len(encoded_to_movie)
print(num_movie)
 
# Mengubah rating menjadi nilai float
data['rating'] = data['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(data['rating'])
 
# Nilai maksimal rating
max_rating = max(data['rating'])
 
print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

# Kita gabungin lah si user dan movie
x = data[['user', 'movie']].values


#Variabel y untuk rating dari hasil
y = data['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

#bagi data jadi 80%
deep_learning = int(0.8 * data.shape[0])
x_train, x_val, y_train, y_val = (
    x[:deep_learning],
    x[deep_learning:],
    y[:deep_learning],
    y[deep_learning:]
)

"""## Modeling dan Result

### Modeling for Cosine Similarity
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_model = cosine_similarity(df_cosine)
cosine_sim = pd.DataFrame(cosine_model, index = df_movie['title'], columns = df_movie['title'])

print('Shape:', cosine_sim.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim.sample(5, axis=1).sample(10, axis=0)

def resto_recommendations(nama_resto, similarity_data=cosine_sim, items = df_movie[['title', 'genres', 'Adventure', 'Animation', 'Children',
       'Comedy', 'Fantasy', 'Romance', 'Drama', 'Action', 'Crime', 'Thriller',
       'Horror', 'Mystery', 'Sci-Fi', 'War', 'Musical', 'Documentary', 'IMAX',
       'Western', 'Film-Noir', '(no genres listed)']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_resto].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_resto, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""#### Testing and Evaluationg the Model"""

movie = resto_recommendations('The Darkest Minds (2018)')
movie

retro = df_movie[df_movie['title'] == 'The Darkest Minds (2018)']
get_genre = [i for i in genres if retro[i].values == 1]
df_movie[df_movie['title'] == 'The Darkest Minds (2018)']["Sci-Fi"].values[0]

def accurate (name ):
  retro = df_movie[df_movie['title'] == name]
  get_genre = [i for i in genres if retro[i].values == 1]
  sum = float(0)
  for j in get_genre :
    print("The accuracy of "+ j+" : " + str((movie[j].sum()/len(movie[j]))*100) + "%")

accurate('The Darkest Minds (2018)')

"""### Model Training for Deep Learning"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers



class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings resto
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_resto = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_resto + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 25,
    validation_data = (x_val, y_val),
)

"""#### Testing and Evaluation the Model"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

movie_df = df_movie

 
# Mengambil sample user
user_id = data.userId.sample(1).iloc[0]
movie_visited_by_user = data[data.userId == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
movie_not_visited = movie_df[~movie_df['movieId'].isin(movie_visited_by_user.movieId.values)]['movieId'] 
movie_not_visited = list(
    set(movie_not_visited)
    .intersection(set(movie_to_encoded.keys()))
)
 
movie_not_visited = [[movie_to_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
 
top_movie_recommend = (
    movie_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = df_movie[df_movie['movieId'].isin(top_movie_recommend)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)
 
print('----' * 8)
print('Top 10 Moive')
print('----' * 8)
 
recommended_movie = df_movie[df_movie['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)